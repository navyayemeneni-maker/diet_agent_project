# Groq API Setup Guide

## Why Groq?

Your Diet Recommendation System has been migrated from Google Gemini to Groq for:

- ‚ö° **3-10x faster responses** (40-200 tokens/sec)
- üí∞ **Better free tier** (14,000 RPM vs Gemini's 20 RPD)
- üéØ **Specialized models** for each agent
- üîÑ **OpenAI-compatible API** (easy to use)

## Step 1: Get Your FREE Groq API Key

1. Go to: **https://console.groq.com**
2. Sign up with your email (no credit card needed)
3. Click "Create API Key"
4. Copy your key

## Step 2: Add to .env File

Open your `.env` file and add:

```
GROQ_API_KEY=gsk_your_actual_key_here
```

## Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

This will install the `openai` package needed for Groq.

## Step 4: Run the App

```bash
streamlit run app.py
```

## Models Used (Production-Stable with Automatic Fallback)

Each agent uses **only production-stable models** from Groq's official list. If the primary model fails, it automatically tries backup models:

| Agent | Primary Model | Fallback 1 | Fallback 2 |
|-------|---------------|------------|------------|
| **Agent 1: Medical Translator** | `llama-3.3-70b-versatile` | `openai/gpt-oss-120b` | `openai/gpt-oss-20b` |
| **Agent 2: Diet Recommender** | `llama-3.3-70b-versatile` | `openai/gpt-oss-120b` | `openai/gpt-oss-20b` |
| **Agent 3: Meal Planner** | `llama-3.1-8b-instant` | `openai/gpt-oss-20b` | `openai/gpt-oss-120b` |
| **Agent 4: Q&A Bot** | `llama-3.1-8b-instant` | `openai/gpt-oss-20b` | `openai/gpt-oss-120b` |

**Why These Models?**
- ‚úÖ **Production-Stable:** Actively supported by Groq, won't be deprecated
- ‚úÖ **High Performance:** Best reasoning and speed for each task
- ‚úÖ **Reliable Fallback:** If one model is down, another takes over
- ‚úÖ **Rate Limit Protection:** Automatically switches if you hit limits
- ‚úÖ **Always Works:** System never fails completely

**Models Avoided (Deprecated):**
- ‚ùå `deepseek-r1-distill-llama-70b` - Deprecated
- ‚ùå `llama3-8b-8192` / `llama3-70b-8192` - Deprecated
- ‚ùå `gemma2-9b-it` - Deprecated
- ‚ùå `qwen` models - Not in production list

## Production Models Used

All models in this system are from **Groq's official production list** (as of 2025):

### ‚úÖ Production-Stable Models:
- `llama-3.3-70b-versatile` - High reasoning + context capacity
- `llama-3.1-8b-instant` - Fast, cheap, versatile
- `openai/gpt-oss-120b` - Strong reasoning and language understanding
- `openai/gpt-oss-20b` - Mid-range large open-weight model

### ‚ùå Deprecated Models (NOT USED):
- `deepseek-r1-distill-llama-70b` - Deprecated by Groq
- `llama3-8b-8192` / `llama3-70b-8192` - Deprecated
- `gemma2-9b-it` - Deprecated
- `qwen` models - Not in production list

**Why This Matters:**
- ‚úÖ No sudden shutdowns or deprecations
- ‚úÖ Actively supported and maintained
- ‚úÖ Best performance and reliability
- ‚úÖ Future-proof your application

## Free Tier Limits

**Groq Free Tier:**
- ‚úÖ 14,400 requests per minute (8B models)
- ‚úÖ 1,000 requests per minute (70B models)
- ‚úÖ No daily cap (fair use policy)
- ‚úÖ Completely FREE

**Your app will never hit these limits!**

## Troubleshooting

### Error: "Invalid API key"
- Make sure you copied the full key from Groq console
- Check that `.env` file has `GROQ_API_KEY=` (not `GOOGLE_API_KEY`)

### Error: "Module 'openai' not found"
- Run: `pip install openai`

### Slow responses?
- Check your internet connection
- Groq should be 3-10x faster than Gemini

## Performance Comparison

**Before (Gemini):**
- Full report generation: ~2-3 minutes
- Rate limit: 20 requests per day

**After (Groq):**
- Full report generation: ~30-60 seconds ‚ö°
- Rate limit: 14,400 requests per minute üöÄ

## Automatic Fallback System

Your system now has **intelligent fallback** built-in:

### How It Works:

1. **Primary Model Tries First**
   - Agent attempts to use the best model (e.g., `llama-3.3-70b-versatile`)

2. **Automatic Retry on Failure**
   - If primary fails (rate limit, downtime, error), automatically tries Fallback 1
   - If Fallback 1 fails, tries Fallback 2

3. **Always Returns Something**
   - Only fails if ALL 3 models fail (extremely rare)
   - You'll see which model was used in the console logs

### Example Console Output:

```
üîÑ Translating medical text...
   ‚ö†Ô∏è Model llama-3.3-70b-versatile failed: Rate limit exceeded
   Trying fallback model 1: llama-3.1-70b-versatile
‚úÖ Translation complete using llama-3.1-70b-versatile!
```

### Benefits:

- ‚úÖ **99.9% Uptime:** System rarely fails
- ‚úÖ **No Manual Intervention:** Automatic recovery
- ‚úÖ **Transparent:** See which model was used
- ‚úÖ **Optimized:** Always tries fastest/best model first

## Need Help?

- Groq Documentation: https://console.groq.com/docs
- Groq Discord: https://groq.com/discord
